{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2dd63a9-8c08-478b-a6bd-e70630b41cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded IDs — Entities:13,645 Relations:15 Times:261\n",
      "Parsed edges — train:222,732 valid:9,404 test:10,013\n",
      "Ingesting 222,732 train edges…\n",
      "Ingesting 9,404 valid edges…\n",
      "Ingesting 10,013 test edges…\n",
      "\n",
      "# of FDEntity nodes: 13645\n",
      "# of FDRelation nodes: 15\n",
      "# of FDTime nodes: 261\n",
      "# of {R_FACT} relationships: 439113\n",
      "\n",
      "Top 10 entities by FACT-degree (isolated labels):\n",
      "Donald Trump: 64642\n",
      "United States: 52086\n",
      "China: 36322\n",
      "U.S. Federal Reserve: 33537\n",
      "Joe Biden: 21698\n",
      "COVID-19: 8703\n",
      "Russia: 8415\n",
      "US Government: 7587\n",
      "Meta Platforms Inc.: 6378\n",
      "Apple Inc.: 6203\n",
      "\n",
      "Neighbors of 'Apple Inc.' (limit 10):\n",
      "<Record neighbor='House Speaker Paul Ryan' relation='Relate_To' time='2018-03-18' split='train'>\n",
      "<Record neighbor='Ukrainian Government' relation='2266' time='2022-11-27' split='test'>\n",
      "<Record neighbor='Mr. Macron' relation='10178' time='2022-12-18' split='test'>\n",
      "<Record neighbor='Governor Gavin Newsom' relation='10203' time='2022-12-25' split='test'>\n",
      "<Record neighbor='The Biden Administration' relation='2560' time='2022-10-30' split='test'>\n",
      "<Record neighbor='Elon Musk' relation='1165' time='2022-12-18' split='test'>\n",
      "<Record neighbor='Stripe Inc.' relation='6050' time='2022-11-06' split='test'>\n",
      "<Record neighbor='National Health Commission' relation='2785' time='2022-12-04' split='test'>\n",
      "<Record neighbor='YouGov' relation='3923' time='2022-10-09' split='test'>\n",
      "<Record neighbor='European Parliament' relation='3677' time='2022-10-09' split='test'>\n",
      "\n",
      "Edges with time starting with '2022':\n",
      "<Record head='United Arab Emirates' rel='Relate_To' tail='Donald Trump' time='2022-10-23'>\n",
      "<Record head='Ghana' rel='1564' tail='Donald Trump' time='2022-12-18'>\n",
      "<Record head='China' rel='531' tail='Donald Trump' time='2022-12-04'>\n",
      "<Record head='Dow Jones Industrial Average' rel='132' tail='Donald Trump' time='2022-12-04'>\n",
      "<Record head='Kim Yo Jong' rel='51' tail='Donald Trump' time='2022-12-25'>\n",
      "<Record head='Peter Thiel' rel='1499' tail='Donald Trump' time='2022-12-04'>\n",
      "<Record head='Kevin Spacey' rel='5913' tail='Donald Trump' time='2022-11-20'>\n",
      "<Record head='China' rel='46' tail='Donald Trump' time='2022-12-11'>\n",
      "<Record head='Kwasi Kwarteng' rel='243' tail='Donald Trump' time='2022-10-23'>\n",
      "<Record head='Elon Musk' rel='17' tail='Donald Trump' time='2022-12-25'>\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# ======================\n",
    "# --- CONFIG SECTION ---\n",
    "# ======================\n",
    "NEO4J_URI  = \"neo4j://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASS = \"YourStrongPass123!\"\n",
    "\n",
    "# Path to the directory that holds the FinDKG files\n",
    "# If you run this script from anywhere, point this to your Neo4j project's import dir.\n",
    "DATA_DIR = Path(\"/Users/agniksaha/Downloads/FinDKG-main/FinDKG_dataset/FinDKG-full\")\n",
    "\n",
    "# Batch sizes (tune for speed/memory)\n",
    "NODE_BATCH = 20_000\n",
    "EDGE_BATCH = 20_000\n",
    "\n",
    "# Tag so you can filter queries if you like\n",
    "DATASET_TAG = \"FinDKG\"\n",
    "\n",
    "# Isolated labels / rel type (to avoid collisions with prior data)\n",
    "L_ENTITY = \"FDEntity\"\n",
    "L_REL    = \"FDRelation\"\n",
    "L_TIME   = \"FDTime\"\n",
    "R_FACT   = \"FD_FACT\"\n",
    "\n",
    "# =================================\n",
    "# --- FILE READERS (exact fit) ---\n",
    "# =================================\n",
    "def read_time2id_csv(fp: Path) -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    time2id.txt format (CSV):\n",
    "      TimeID,DATE_WK\n",
    "      0,2018-01-07\n",
    "      ...\n",
    "    \"\"\"\n",
    "    out: Dict[int, str] = {}\n",
    "    if not fp.exists():\n",
    "        return out\n",
    "    with fp.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        for row in rdr:\n",
    "            tid_s = row.get(\"TimeID\") or row.get(\"timeid\") or row.get(\"time_id\")\n",
    "            date_s = row.get(\"DATE_WK\") or row.get(\"date\") or row.get(\"DATE\")\n",
    "            if tid_s is None or date_s is None:\n",
    "                continue\n",
    "            try:\n",
    "                tid = int(tid_s.strip())\n",
    "            except ValueError:\n",
    "                continue\n",
    "            out[tid] = date_s.strip()\n",
    "    return out\n",
    "\n",
    "def read_relation2id_tsv(fp: Path) -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    relation2id.txt format (TSV):\n",
    "      Relate_To<TAB>0\n",
    "      Control<TAB>1\n",
    "      ...\n",
    "    \"\"\"\n",
    "    out: Dict[int, str] = {}\n",
    "    with fp.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            ln = ln.strip()\n",
    "            if not ln:\n",
    "                continue\n",
    "            parts = ln.split(\"\\t\")\n",
    "            if len(parts) != 2:\n",
    "                parts = ln.split()\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "            name, rid_s = parts[0], parts[1]\n",
    "            try:\n",
    "                rid = int(rid_s)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            out[rid] = name\n",
    "    return out\n",
    "\n",
    "def read_entity2id(fp: Path) -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    entity2id.txt format (TSV with 4 columns):\n",
    "      name<TAB>id<TAB>...<TAB>...\n",
    "    Use the FIRST numeric token after name as eid.\n",
    "    \"\"\"\n",
    "    out: Dict[int, str] = {}\n",
    "    with fp.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            ln = ln.rstrip(\"\\n\\r\")\n",
    "            if not ln:\n",
    "                continue\n",
    "            parts = ln.split(\"\\t\") if \"\\t\" in ln else ln.rsplit(maxsplit=3)\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            name = parts[0].strip()\n",
    "            eid_s = None\n",
    "            for tok in parts[1:]:\n",
    "                tok = tok.strip()\n",
    "                if tok.isdigit() or (tok.startswith(\"-\") and tok[1:].isdigit()):\n",
    "                    eid_s = tok\n",
    "                    break\n",
    "            if eid_s is None:\n",
    "                continue\n",
    "            try:\n",
    "                eid = int(eid_s)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            out[eid] = name\n",
    "    return out\n",
    "\n",
    "def read_edges_ids(fp: Path) -> List[Tuple[int, int, int, Optional[int]]]:\n",
    "    \"\"\"\n",
    "    train/valid/test format (IDs only):\n",
    "      h_id  t_id  r_id  [time_id]\n",
    "    separated by TAB or SPACE.\n",
    "    \"\"\"\n",
    "    edges: List[Tuple[int, int, int, Optional[int]]] = []\n",
    "    with fp.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            ln = ln.strip()\n",
    "            if not ln:\n",
    "                continue\n",
    "            parts = ln.split(\"\\t\")\n",
    "            if len(parts) < 3:\n",
    "                parts = ln.split()\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            try:\n",
    "                h = int(parts[0]); t = int(parts[1]); r = int(parts[2])\n",
    "                tm = int(parts[3]) if len(parts) >= 4 and parts[3] != \"\" else None\n",
    "            except ValueError:\n",
    "                raise ValueError(f\"{fp.name} must contain numeric IDs only (got: {ln[:80]}...)\")\n",
    "            edges.append((h, t, r, tm))\n",
    "    return edges\n",
    "\n",
    "# ==================================\n",
    "# --- CYPHER / INGESTION ROUTES ---\n",
    "# ==================================\n",
    "def create_constraints(driver):\n",
    "    with driver.session() as session:\n",
    "        # Only on our isolated labels\n",
    "        session.run(f\"CREATE CONSTRAINT IF NOT EXISTS FOR (e:{L_ENTITY}) REQUIRE e.eid IS UNIQUE;\")\n",
    "        session.run(f\"CREATE CONSTRAINT IF NOT EXISTS FOR (r:{L_REL})    REQUIRE r.rid IS UNIQUE;\")\n",
    "        session.run(f\"CREATE CONSTRAINT IF NOT EXISTS FOR (t:{L_TIME})   REQUIRE t.tid IS UNIQUE;\")\n",
    "        # Neo4j 5+: unique key on relationship\n",
    "        try:\n",
    "            session.run(f\"CREATE CONSTRAINT IF NOT EXISTS FOR ()-[f:{R_FACT}]-() REQUIRE f.key IS UNIQUE;\")\n",
    "        except Exception as e:\n",
    "            print(\"Note: FD_FACT(key) uniqueness not created (likely Neo4j < 5):\", e, file=sys.stderr)\n",
    "\n",
    "def ingest_entities(driver, id2name: Dict[int, str]):\n",
    "    rows = [{\"eid\": k, \"name\": v, \"dataset\": DATASET_TAG} for k, v in id2name.items()]\n",
    "    for i in range(0, len(rows), NODE_BATCH):\n",
    "        chunk = rows[i:i+NODE_BATCH]\n",
    "        with driver.session() as session:\n",
    "            session.run(f\"\"\"\n",
    "                UNWIND $rows AS row\n",
    "                MERGE (e:{L_ENTITY} {{eid: row.eid}})\n",
    "                ON CREATE SET e.name = row.name, e.dataset = row.dataset\n",
    "                ON MATCH  SET e.name = coalesce(e.name, row.name),\n",
    "                              e.dataset = coalesce(e.dataset, row.dataset)\n",
    "            \"\"\", rows=chunk)\n",
    "\n",
    "def ingest_relations(driver, id2name: Dict[int, str]):\n",
    "    rows = [{\"rid\": k, \"name\": v, \"dataset\": DATASET_TAG} for k, v in id2name.items()]\n",
    "    for i in range(0, len(rows), NODE_BATCH):\n",
    "        chunk = rows[i:i+NODE_BATCH]\n",
    "        with driver.session() as session:\n",
    "            session.run(f\"\"\"\n",
    "                UNWIND $rows AS row\n",
    "                MERGE (r:{L_REL} {{rid: row.rid}})\n",
    "                ON CREATE SET r.name = row.name, r.dataset = row.dataset\n",
    "                ON MATCH  SET r.name = coalesce(r.name, row.name),\n",
    "                              r.dataset = coalesce(r.dataset, row.dataset)\n",
    "            \"\"\", rows=chunk)\n",
    "\n",
    "def ingest_times(driver, id2date: Dict[int, str]):\n",
    "    if not id2date:\n",
    "        return\n",
    "    rows = [{\"tid\": k, \"value\": v, \"dataset\": DATASET_TAG} for k, v in id2date.items()]\n",
    "    for i in range(0, len(rows), NODE_BATCH):\n",
    "        chunk = rows[i:i+NODE_BATCH]\n",
    "        with driver.session() as session:\n",
    "            session.run(f\"\"\"\n",
    "                UNWIND $rows AS row\n",
    "                MERGE (t:{L_TIME} {{tid: row.tid}})\n",
    "                ON CREATE SET t.value = row.value, t.dataset = row.dataset\n",
    "                ON MATCH  SET t.value = coalesce(t.value, row.value),\n",
    "                              t.dataset = coalesce(t.dataset, row.dataset)\n",
    "            \"\"\", rows=chunk)\n",
    "\n",
    "def ingest_edges(driver,\n",
    "                 edges: List[Tuple[int, int, int, Optional[int]]],\n",
    "                 split_name: str,\n",
    "                 rid2name: Dict[int, str],\n",
    "                 tid2date: Dict[int, str]):\n",
    "    \"\"\"\n",
    "    One relationship type (:FD_FACT) with a unique 'key' so we can MERGE without duplicates.\n",
    "    Properties: rid, rel, tid, time, split, dataset\n",
    "    \"\"\"\n",
    "    if not edges:\n",
    "        print(f\"No {split_name} edges to ingest.\")\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    for (h, t, r, tm) in edges:\n",
    "        tm_str = \"NA\" if tm is None else str(tm)\n",
    "        key = f\"{h}|{t}|{r}|{tm_str}|{split_name}|{DATASET_TAG}\"\n",
    "        rows.append({\n",
    "            \"h\": h, \"t\": t,\n",
    "            \"rid\": r, \"rel\": rid2name.get(r, str(r)),\n",
    "            \"tid\": tm, \"time\": (tid2date.get(tm) if tm is not None else None),\n",
    "            \"split\": split_name, \"dataset\": DATASET_TAG, \"key\": key\n",
    "        })\n",
    "\n",
    "    print(f\"Ingesting {len(rows):,} {split_name} edges…\")\n",
    "    for i in range(0, len(rows), EDGE_BATCH):\n",
    "        chunk = rows[i:i+EDGE_BATCH]\n",
    "        with driver.session() as session:\n",
    "            session.run(f\"\"\"\n",
    "                UNWIND $rows AS row\n",
    "                MATCH (h:{L_ENTITY} {{eid: row.h}})\n",
    "                MATCH (t:{L_ENTITY} {{eid: row.t}})\n",
    "                MERGE (h)-[e:{R_FACT} {{key: row.key}}]->(t)\n",
    "                ON CREATE SET e.rid=row.rid, e.rel=row.rel, e.tid=row.tid, e.time=row.time,\n",
    "                              e.split=row.split, e.dataset=row.dataset\n",
    "                ON MATCH  SET e.rid=coalesce(e.rid,row.rid),\n",
    "                              e.rel=coalesce(e.rel,row.rel),\n",
    "                              e.tid=coalesce(e.tid,row.tid),\n",
    "                              e.time=coalesce(e.time,row.time),\n",
    "                              e.split=coalesce(e.split,row.split),\n",
    "                              e.dataset=coalesce(e.dataset,row.dataset)\n",
    "            \"\"\", rows=chunk)\n",
    "\n",
    "# ==========================\n",
    "# --- SANITY QUERIES -------\n",
    "# ==========================\n",
    "def sample_queries(driver):\n",
    "    with driver.session() as session:\n",
    "        print(\"\\n# of FDEntity nodes:\",\n",
    "              session.run(f\"MATCH (e:{L_ENTITY}) RETURN count(e) AS n\").single()[\"n\"])\n",
    "        print(\"# of FDRelation nodes:\",\n",
    "              session.run(f\"MATCH (r:{L_REL}) RETURN count(r) AS n\").single()[\"n\"])\n",
    "        print(\"# of FDTime nodes:\",\n",
    "              session.run(f\"MATCH (t:{L_TIME}) RETURN count(t) AS n\").single()[\"n\"])\n",
    "        print(\"# of {R_FACT} relationships:\",\n",
    "              session.run(f\"MATCH ()-[f:{R_FACT}]-() RETURN count(f) AS n\").single()[\"n\"])\n",
    "\n",
    "        print(\"\\nTop 10 entities by FACT-degree (isolated labels):\")\n",
    "        for rec in session.run(f\"\"\"\n",
    "            MATCH (e:{L_ENTITY})\n",
    "            OPTIONAL MATCH (e)-[f:{R_FACT}]-()\n",
    "            WITH e, count(f) AS deg\n",
    "            RETURN e.name AS entity, deg\n",
    "            ORDER BY deg DESC LIMIT 10\n",
    "        \"\"\"):\n",
    "            print(f\"{rec['entity']}: {rec['deg']}\")\n",
    "\n",
    "        target = \"Apple Inc.\"\n",
    "        print(f\"\\nNeighbors of '{target}' (limit 10):\")\n",
    "        for rec in session.run(f\"\"\"\n",
    "            MATCH (e:{L_ENTITY} {{name:$name}})\n",
    "            OPTIONAL MATCH (e)-[r:{R_FACT}]-(nbr:{L_ENTITY})\n",
    "            RETURN nbr.name AS neighbor, r.rel AS relation, r.time AS time, r.split AS split\n",
    "            LIMIT 10\n",
    "        \"\"\", name=target):\n",
    "            print(rec)\n",
    "\n",
    "        year_prefix = \"2022\"\n",
    "        print(f\"\\nEdges with time starting with '{year_prefix}':\")\n",
    "        for rec in session.run(f\"\"\"\n",
    "            MATCH (h:{L_ENTITY})-[r:{R_FACT}]->(t:{L_ENTITY})\n",
    "            WHERE r.time IS NOT NULL AND r.time STARTS WITH $pref\n",
    "            RETURN h.name AS head, r.rel AS rel, t.name AS tail, r.time AS time\n",
    "            LIMIT 10\n",
    "        \"\"\", pref=year_prefix):\n",
    "            print(rec)\n",
    "\n",
    "# ===============\n",
    "# --- MAIN ------\n",
    "# ===============\n",
    "def main():\n",
    "    assert DATA_DIR.exists(), f\"DATA_DIR not found: {DATA_DIR}\"\n",
    "\n",
    "    # Read IDs in their true formats\n",
    "    time_map     = read_time2id_csv(DATA_DIR / \"time2id.txt\")          # {tid -> date_str}\n",
    "    relation_map = read_relation2id_tsv(DATA_DIR / \"relation2id.txt\")  # {rid -> name}\n",
    "    entity_map   = read_entity2id(DATA_DIR / \"entity2id.txt\")          # {eid -> name}\n",
    "\n",
    "    print(f\"Loaded IDs — Entities:{len(entity_map):,} Relations:{len(relation_map):,} Times:{len(time_map):,}\")\n",
    "\n",
    "    # Read splits (IDs only). If this errors, your split files are names — ping me and I’ll swap readers.\n",
    "    train_edges = read_edges_ids(DATA_DIR / \"train.txt\")\n",
    "    valid_edges = read_edges_ids(DATA_DIR / \"valid.txt\")\n",
    "    test_edges  = read_edges_ids(DATA_DIR / \"test.txt\")\n",
    "    print(f\"Parsed edges — train:{len(train_edges):,} valid:{len(valid_edges):,} test:{len(test_edges):,}\")\n",
    "\n",
    "    # Connect + ingest into isolated labels\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))\n",
    "    try:\n",
    "        create_constraints(driver)\n",
    "        ingest_entities(driver, entity_map)\n",
    "        ingest_relations(driver, relation_map)\n",
    "        ingest_times(driver, time_map)\n",
    "        ingest_edges(driver, train_edges, \"train\", relation_map, time_map)\n",
    "        ingest_edges(driver, valid_edges, \"valid\", relation_map, time_map)\n",
    "        ingest_edges(driver, test_edges,  \"test\",  relation_map, time_map)\n",
    "        sample_queries(driver)\n",
    "    finally:\n",
    "        driver.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e0706-f9a3-4272-a1eb-87b4fc912099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
